import sys
import os
import datetime
import hashlib
import subprocess

from django.core import exceptions
from django.conf import settings

from apps.process_monitor.sipproc import SipProcess
from apps.base_item import models as base_item

import models

try:
    TREE_IS_INGESTION_SVN = settings.TREE_IS_INGESTION_SVN
except:
    TREE_IS_INGESTION_SVN = False

IMPORT_SCAN_TREE = settings.IMPORT_SCAN_TREE

AUTOGEN_NAME = 'Autogenerated by Sip Manager'

class RequestCreate(SipProcess):
    SHORT_DESCRIPTION = 'Scan for new imports, if found grab it'
    PROVIDER_TYPE_LOOKUP = {'L': models.PROVT_LIBRARY,
                            'A':models.PROVT_ARCHIVE,
                            'M': models.PROVT_MUSEUM,
                            'AA': models.PROVT_AUDIO_VIS_ARCH,
                            'AG': models.PROVT_AGGREGATOR,
                            }

    def __init__(self, *args, **kwargs):
        self.skip_dirs = ['error', 'to-import', 'to-import-cache-only',
                          'to-import-no-cache', 'uploading', '.svn']
        super(RequestCreate, self).__init__(*args, **kwargs)

    def run_it(self):
        """
        Dirs to be scanned:
            finished
            imported
            importing
            nonexistent
            uploaded
            validated
            validating

        Indicating a deletion:
            error

        Avoid the following:
            to-import
            to-import-cache-only
            to-import-no-cache
            uploading
        """
        skip_trees = [] # if we find a subtree that shouldnt be followed like .svn dont dive into it
        for dirpath, dirnames, filenames in os.walk(IMPORT_SCAN_TREE):
            if dirpath == IMPORT_SCAN_TREE:
                continue
            b = False
            for bad_tree in skip_trees:
                if bad_tree in dirpath:
                    b = True
                    break
            if b:
                continue

            # avoid specific relative dirs that shouldnt be traversed
            if os.path.split(dirpath)[-1] in self.skip_dirs:
                skip_trees.append(dirpath)
                continue

            # if we are scanning the ingestion svn only /.../output_xml/ should be used
            if TREE_IS_INGESTION_SVN and os.path.split(dirpath)[-1] != 'output_xml':
                continue

            for filename in filenames:
                if os.path.splitext(filename)[1] != '.xml':
                    continue
                # if we are scanning the ingestion svn avoid things like 'dddd.sample.xml'
                if TREE_IS_INGESTION_SVN and os.path.splitext(os.path.splitext(filename)[0])[1] != '':
                    # extra check needed for using ingestion svn tree
                    continue

                # we (hopefully) have a relevant file, check if it is already ingested
                data_sets = models.DataSet.objects.filter(name_code=filename)
                if data_sets:
                    data_set = data_sets[0]
                else:
                    # in normal operation we should propably abort here,
                    # Datasets should propably be pre-created by ingestion
                    data_set = self.add_data_set(filename)
                    if not data_set:
                        continue # wasnt a valid dataset

                full_path = os.path.join(dirpath, filename)
                mtime = os.path.getmtime(full_path)
                time_created = datetime.datetime.fromtimestamp(mtime)
                requests = models.Request.objects.filter(data_set=data_set,
                                                         time_created=time_created,
                                                         file_name=filename)
                if not requests:
                    # only add new requests
                    record_count = self.record_count(full_path)
                    request = models.Request().add_from_file(data_set, full_path, record_count)
                    self.log('Added request %s' % filename, 3)
                pass
        return True

    def add_data_set(self, file_name):
        try:
            int(file_name[:5]) # we use it as string but check that it is an int...
        except:
            return None # not a valid dataset

        aggregator_nc = file_name[:2]
        aggregators = models.Aggregator.objects.filter(name_code=aggregator_nc)
        if aggregators:
            aggregator = aggregators[0]
        else:
            aggregator = models.Aggregator(name_code=aggregator_nc, name=AUTOGEN_NAME)
            aggregator.save()

        provider_nc = file_name[2:5]
        providers = models.Provider.objects.filter(name_code=provider_nc)
        if providers:
            provider = providers[0]
        else:
            try:
                sanitized_file_name = os.path.splitext(file_name)[0].replace('__','_')
                s = sanitized_file_name.split('_')[1].upper()
                item_type = self.PROVIDER_TYPE_LOOKUP[s]
                country_idx = 2
            except:
                item_type = 1
                country_idx= 1
            try:
                country = sanitized_file_name.split('_')[country_idx]
                if len(country) > 2:
                    raise
            except:
                country = '??'
            try:
                name = ' '.join(sanitized_file_name.split('_')[country_idx+1:])
            except:
                name = AUTOGEN_NAME
            provider = models.Provider(aggregator=aggregator,
                                       name_code=provider_nc,
                                       name=name,
                                       item_type=item_type,
                                       country=country)
            provider.save()

        data_sets = models.DataSet.objects.filter(name_code=file_name)
        if data_sets:
            data_set = data_sets[0]
        else:
            data_set = models.DataSet(provider=provider,
                                      name_code=file_name,
                                      name=AUTOGEN_NAME,
                                      language='??')
            data_set.save()

        return data_set

    def record_count(self, fname):
        #cmd = "awk 'BEGIN {RS=FS}{if ( $0 ~ /<record>/ ) c++} END{print c}' %s" % fname
        cmd = 'awk -F "<record>" \'{s+=(NF-1)} END {print s}\' %s' % fname
        self.log('counting records in %s' % fname, 9)
        p = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE,
                             stderr=subprocess.PIPE, close_fds=True)
        retcode = p.wait()
        if retcode:
            self.abort_process('shell command counting records in xml-file failed: %s' % fname)
        rec_count = p.stdout.read().strip()
        self.log('\tfound records: %s' % rec_count, 9)
        return rec_count



class RequestParseNew(SipProcess):
    SHORT_DESCRIPTION = 'Parse new processes'

    def run_it(self):
        """
        for mdRecord in MdRecord.items.all():
            if not Uri.items.filter(md_rec_id=mdRecord.id):
                u = Uri(md_rec_id=mdRecord.id)
                u.save()
        """
        requests = models.Request.objects.filter(status=models.REQS_INIT,
                                                 pid=0)
        if not requests:
            return True # no pending requests
        for request in requests:
            if not request.pid:
                self.handle_request(requests[0])
        return True

    def handle_request(self, request):
        full_path = self.find_file(request)
        if not full_path:
            self.error_log('Cant find file %s for Request %i' % (
                request.file_name, request.pk))
            return False
        request, process_manager = self.grab_item(models.Request, request.pk,
                                                  'About to parse for ese records')
        if not request:
            return False

        self.current_request = request
        self.log('Parsing ese file for records: %s' % full_path, 1)
        f = open(full_path)
        record_count = 0
        record = []
        for line_raw in f.readlines():
            line = line_raw.strip()
            if line == '<record>':
                record = []
            elif line == '</record>':
                self.add_record(record)
            elif line: # skip empty lines
                record.append(line_raw)
            pass
        f.close()

    def add_record(self, record):
        r_hash = self.calculate_hash(record)
        mdrs = base_item.MdRecord.objects.filter(content_hash=r_hash)
        if mdrs:
            mdr= mdrs[0]
        else:
            mdr = base_item.MdRecord(content_hash=r_hash,source_data=unicode(''.join(record),'utf-8'))
            mdr.save()
        r_m = base_item.RequestMdRecord(request=self.current_request,
                                         md_record=mdr)
        r_m.save()
        sys.stdout.write('.')
        sys.stdout.flush()

    def calculate_hash(self, record):
        lst = []
        for line in record:
            lst.append(line.strip())
        r_hash = hashlib.sha256(''.join(lst)).hexdigest().upper()
        return r_hash

    def find_file(self, request):
        full_path = ''
        found = False
        for dirpath, dirnames, filenames in os.walk(IMPORT_SCAN_TREE):
            if found:
                break
            for filename in filenames:
                if filename == request.file_name:
                    full_path = os.path.join(dirpath, filename)
                    mtime = os.path.getmtime(full_path)
                    time_created = datetime.datetime.fromtimestamp(mtime)
                    if request.time_created == time_created:
                        found = True
                        break
                    pass
            pass
        return full_path




task_list = [RequestCreate]#, RequestParseNew]
